{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CSDMS_Unet_BinarySeg_Buscombe.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/zaidalyafeai/Notebooks/blob/master/unet.ipynb","timestamp":1557783978307}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mh-MgyQM2Qw9","colab_type":"text"},"source":["# Binary Segmentation using U-Net\n","\n","CSDMS\t2019\tAnnual\tMeeting:\tCSDMS\t3.0\tâ€“ Bridging\tBoundaries\n","\n","May\t21-23,\t2019\n","\n","### Daniel Buscombe\n","\n","Assistant Research Professor\n","\n","School of Earth and Sustainability\n","\n","School of Informatics, Computing and Cyber Systems\n","\n","\n","Northern Arizona University, Flagstaff, AZ\n","\n","[Email](mailto:daniel.buscombe@nau.edu)\n","[Web](www.danielbuscombe.com)\n","[Google Scholar](https://scholar.google.com/citations?user=bwVl0NwAAAAJ&hl=en)\n","\n","\n","## Introduction\n","\n","The U-Net model is a simple fully  convolutional neural network that is used for binary segmentation i.e foreground and background pixel-wise classification. Mainly, it consists of two parts. \n","\n","*   Encoder: we apply a series of conv layers and downsampling layers  (max-pooling) layers to reduce the spatial size \n","*   Decoder: we apply a series of upsampling layers to reconstruct the spatial size of the input. \n","\n","The two parts are connected using a concatenation layers among different levels. This allows learning different features at different levels. At the end we have a simple conv 1x1 layer to reduce the number of channels to 1. \n","\n","\n","![alt text](https://blog.playment.io/wp-content/uploads/2018/03/Screen-Shot-2018-09-05-at-3.00.03-PM.png)"]},{"cell_type":"markdown","metadata":{"id":"3LisvDshzevQ","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"bdeDl5HO0QsY","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from PIL import Image\n","import keras\n","from keras.models import Model\n","from keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D\n","from keras.layers import  Dropout, Activation\n","from keras.optimizers import Adam, SGD\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n","from keras import backend as K\n","from keras.utils import plot_model\n","import tensorflow as tf\n","import random\n","import cv2\n","from random import shuffle\n","from glob import glob\n","from imageio import imread, imwrite"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOpRsS-TzhRg","colab_type":"text"},"source":["## Dataset\n","\n","\n","Download the data set from github"]},{"cell_type":"code","metadata":{"id":"db1k6m9yDP6z","colab_type":"code","colab":{}},"source":["! rm -rf Unet_binary_example"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNwAgCNjCuKJ","colab_type":"code","colab":{}},"source":["! git clone --depth 1 https://github.com/dbuscombe-usgs/Unet_binary_example.git"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0t1wcmf7DA5E","colab_type":"text"},"source":["Change directory"]},{"cell_type":"code","metadata":{"id":"YBdYCxwCDA_g","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('./Unet_binary_example')\n","print(os.getcwd())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gF9UZdcr0Xkq","colab_type":"text"},"source":["Note that we have two folders. The first one is `images` which contains the raw images and `annotations` which contains the `labels`imagery, and an empty `masks` folder"]},{"cell_type":"code","metadata":{"id":"3Nag-lQnKcPI","colab_type":"code","colab":{}},"source":["files = glob('annotations/labels/*.tif')\n","len(files)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MYiHpbmLp3K","colab_type":"code","colab":{}},"source":["labs = np.genfromtxt('annotations/class_dict.csv', delimiter=',', skip_header=1, dtype=None)\n","print(labs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOxma2nwcFb6","colab_type":"text"},"source":["Next, define the class that we want to treat as the foreground in the binary segmentation"]},{"cell_type":"code","metadata":{"id":"o8Y2ofnKNIc3","colab_type":"code","colab":{}},"source":["class_foregrnd = 'water' #'veg'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P-Ax2ni5cM8K","colab_type":"text"},"source":["The following code creates binary masks by labeling all water pixels 1, and all others 0, and writes them to the `masks` folder"]},{"cell_type":"code","metadata":{"id":"W0FXC-DSMnGY","colab_type":"code","colab":{}},"source":["index = np.where(np.asarray([k[0].decode()==class_foregrnd for k in labs])==True)[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNWtJrb-Kyxm","colab_type":"code","colab":{}},"source":["for k in range(len(files)):\n","  i0 = (imread(files[k])[:,:,0]==int(labs[index][0][1])).astype('uint8')\n","  i1 = (imread(files[k])[:,:,1]==int(labs[index][0][2])).astype('uint8')\n","  i2 = (imread(files[k])[:,:,2]==int(labs[index][0][3])).astype('uint8')\n","  mask = ((i0+i1+i2)==3).astype('uint8')\n","  imwrite(files[k].replace('labels', 'masks'), mask)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LsBYthq8ci5a","colab_type":"text"},"source":["Now let's take a look at the contents of the `masks` folder. There should be the same number of images as in the `labels` folder"]},{"cell_type":"code","metadata":{"id":"OTJ6Kk01Ovkv","colab_type":"code","colab":{}},"source":["files = glob('annotations/masks/*.tif')\n","len(files)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tavQ2-MJ0x9E","colab_type":"text"},"source":["## Setting up model training\n","\n","The next function creates a generator that will randomly draw images from the training and testing sets"]},{"cell_type":"code","metadata":{"id":"P8tq55g677wp","colab_type":"code","colab":{}},"source":["def image_generator(files, batch_size = 32, sz = (512, 512)):\n","  \n","  while True: \n","    \n","    #extract a random batch \n","    batch = np.random.choice(files, size = batch_size)    \n","    \n","    #variables for collecting batches of inputs and outputs \n","    batch_x = []\n","    batch_y = []\n","    \n","    \n","    for f in batch:\n","\n","        #get the masks. Note that masks are png files \n","        mask = Image.open(glob(f'annotations/masks/{f[:-4]}*.tif')[0])\n","        mask = np.array(mask.resize(sz))\n","\n","\n","        #preprocess the mask \n","        mask[mask >= 2] = 0 \n","        mask[mask != 0 ] = 1\n","        \n","        batch_y.append(mask)\n","\n","        #preprocess the raw images \n","        raw = Image.open(f'images/{f}')\n","        raw = raw.resize(sz)\n","        raw = np.array(raw)\n","\n","        #check the number of channels because some of the images are RGBA or GRAY\n","        if len(raw.shape) == 2:\n","          raw = np.stack((raw,)*3, axis=-1)\n","\n","        else:\n","          raw = raw[:,:,0:3]\n","\n","        batch_x.append(raw)\n","\n","    #preprocess a batch of images and masks \n","    batch_x = np.array(batch_x)/255.\n","    batch_y = np.array(batch_y)\n","    batch_y = np.expand_dims(batch_y,3)\n","\n","    yield (batch_x, batch_y)      \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9yNsHpW6c-sM","colab_type":"text"},"source":["Next we set our batch size, and the proportion of all images to use for training"]},{"cell_type":"code","metadata":{"id":"PvwbmS-YTHEZ","colab_type":"code","colab":{}},"source":["batch_size = 3\n","\n","prop_train = 0.8\n","\n","all_files = os.listdir('images')\n","shuffle(all_files)\n","\n","split = int(prop_train * len(all_files))\n","\n","#split into training and testing\n","train_files = all_files[0:split]\n","test_files  = all_files[split:]\n","\n","train_generator = image_generator(train_files, batch_size = batch_size)\n","test_generator  = image_generator(test_files, batch_size = batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LLTaIQVNEgxl","colab_type":"code","colab":{}},"source":["print(len(train_files))\n","print(len(test_files))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EB4pQy-dMOs","colab_type":"text"},"source":["The next few lines randomly select an image and display it, the associated mask, and the masked image. Rerun this cell again and again to see a different image every time"]},{"cell_type":"code","metadata":{"id":"jnuNW20FeupL","colab_type":"code","colab":{}},"source":["x, y= next(train_generator)\n","plt.axis('off')\n","img = x[0]\n","msk = y[0].squeeze()\n","msk = np.stack((msk,)*3, axis=-1)\n","nx, ny,nz = np.shape(msk)\n","\n","plt.imshow( np.concatenate([img, msk, img*msk], axis = 1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"raXLOfBk1pLV","colab_type":"text"},"source":["## IoU metric\n","\n","The intersection over union (IoU) metric is a simple metric used to evaluate the performance of a segmentation algorithm. Given two masks $y_{true}, y_{pred}$ we evaluate \n","\n","$$IoU = \\frac{y_{true} \\cap y_{pred}}{y_{true} \\cup y_{pred}}$$"]},{"cell_type":"code","metadata":{"id":"xJLriYXX1oZU","colab_type":"code","colab":{}},"source":["def mean_iou(y_true, y_pred):\n","    yt0 = y_true[:,:,:,0]\n","    yp0 = K.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n","    inter = tf.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n","    union = tf.count_nonzero(tf.add(yt0, yp0))\n","    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n","    return iou"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"160g6Ex41r-2","colab_type":"text"},"source":["## U-Net Model"]},{"cell_type":"code","metadata":{"id":"hONrrUbW9CM_","colab_type":"code","colab":{}},"source":["def unet(sz = (512, 512, 3)):\n","  x = Input(sz)\n","  inputs = x\n","  \n","  #down sampling \n","  f = 8\n","  layers = []\n","  \n","  for i in range(0, 6):\n","    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n","    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n","    layers.append(x)\n","    x = MaxPooling2D() (x)\n","    f = f*2\n","  ff2 = 64 \n","  \n","  #bottleneck \n","  j = len(layers) - 1\n","  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n","  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n","  x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n","  x = Concatenate(axis=3)([x, layers[j]])\n","  j = j -1 \n","  \n","  #upsampling \n","  for i in range(0, 5):\n","    ff2 = ff2//2\n","    f = f // 2 \n","    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n","    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n","    x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n","    x = Concatenate(axis=3)([x, layers[j]])\n","    j = j -1 \n","    \n","  \n","  #classification \n","  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n","  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n","  outputs = Conv2D(1, 1, activation='sigmoid') (x)\n","  \n","  #model creation \n","  model = Model(inputs=[inputs], outputs=[outputs])\n","  model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = [mean_iou])\n","  \n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"67Fyeczk_zzh","colab_type":"code","colab":{}},"source":["model = unet()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t7QY8rgO1zUU","colab_type":"text"},"source":["## Callbacks\n","\n","Simple functions to save the model at each epoch and show some predictions "]},{"cell_type":"code","metadata":{"id":"xfqXmNuc9lWZ","colab_type":"code","colab":{}},"source":["def build_callbacks():\n","        checkpointer = ModelCheckpoint(filepath='unet.h5', verbose=0, save_best_only=True, save_weights_only=True)\n","        callbacks = [checkpointer, PlotLearning()]\n","        return callbacks\n","\n","# inheritance for training process plot \n","class PlotLearning(keras.callbacks.Callback):\n","\n","    def on_train_begin(self, logs={}):\n","        self.i = 0\n","        self.x = []\n","        self.losses = []\n","        self.val_losses = []\n","        self.acc = []\n","        self.val_acc = []\n","        #self.fig = plt.figure()\n","        self.logs = []\n","    def on_epoch_end(self, epoch, logs={}):\n","        self.logs.append(logs)\n","        self.x.append(self.i)\n","        self.losses.append(logs.get('loss'))\n","        self.val_losses.append(logs.get('val_loss'))\n","        self.acc.append(logs.get('mean_iou'))\n","        self.val_acc.append(logs.get('val_mean_iou'))\n","        self.i += 1\n","        print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'mean_iou=',logs.get('mean_iou'),'val_mean_iou=',logs.get('val_mean_iou'))\n","        \n","        #choose a random test image and preprocess\n","        path = np.random.choice(test_files)\n","        raw = Image.open(f'images/{path}')\n","        raw = np.array(raw.resize((512, 512)))/255.\n","        raw = raw[:,:,0:3]\n","        \n","        #predict the mask \n","        pred = model.predict(np.expand_dims(raw, 0))\n","        \n","        #mask post-processing \n","        msk  = pred.squeeze()\n","        msk = np.stack((msk,)*3, axis=-1)\n","        msk[msk >= 0.5] = 1 \n","        msk[msk < 0.5] = 0 \n","        \n","        #show the mask and the segmented image \n","        combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n","        plt.axis('off')\n","        plt.imshow(combined)\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sU6SPuVY8Mdc","colab_type":"text"},"source":["## Training\n","\n","We're going to define the number of steps per epoch (based on the batch size) and also the number of epochs"]},{"cell_type":"code","metadata":{"id":"_MXGinNg9Wjj","colab_type":"code","colab":{}},"source":["train_steps = len(train_files) //batch_size\n","test_steps = len(test_files) //batch_size\n","\n","print(train_steps)\n","print(test_steps)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SxVzTrroHbaU","colab_type":"code","colab":{}},"source":["history = model.fit_generator(train_generator, \n","                    epochs = 100, steps_per_epoch = train_steps,validation_data = test_generator, validation_steps = test_steps,\n","                    callbacks = build_callbacks(), verbose = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6yAYM8YNdyPO","colab_type":"text"},"source":["`history` contains the training and validation metrics that we can then plot as a function of epoch"]},{"cell_type":"code","metadata":{"id":"vk16vrT6UmLp","colab_type":"code","colab":{}},"source":["print(history.history.keys())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_z3Q-GPwUnlA","colab_type":"code","colab":{}},"source":["# summarize history for iou\n","plt.plot(history.history['mean_iou'])\n","plt.plot(history.history['val_mean_iou'])\n","plt.title('model IoU')\n","plt.ylabel('IoU')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X65lKDogUt5Q","colab_type":"code","colab":{}},"source":["# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0O5zCKBr8OZ1","colab_type":"text"},"source":["## Testing\n","\n","On an unseen image ..."]},{"cell_type":"code","metadata":{"id":"S17rew7Vqk5e","colab_type":"code","colab":{}},"source":["raw = Image.open('test/buck-farm-vegetation-survey.jpg')\n","raw = np.array(raw.resize((512, 512)))/255.\n","raw = raw[:,:,0:3]\n","\n","#predict the mask \n","pred = model.predict(np.expand_dims(raw, 0))\n","\n","#mask post-processing \n","msk  = pred.squeeze()\n","msk = np.stack((msk,)*3, axis=-1)\n","msk[msk >= 0.5] = 1 \n","msk[msk < 0.5] = 0 \n","\n","#show the mask and the segmented image \n","combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n","plt.axis('off')\n","plt.imshow(combined)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OfoO80D4uYAj","colab_type":"text"},"source":["## References\n","\n","\n","1.   http://deeplearning.net/tutorial/unet.html\n","2.   https://github.com/ldenoue/keras-unet\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zAqUysfEVb_l","colab_type":"text"},"source":["CSDMS\t2019\tAnnual\tMeeting:\tCSDMS\t3.0\tâ€“ Bridging\tBoundaries\n","\n","May\t21-23,\t2019\n","\n","### Daniel Buscombe\n","\n","Assistant Research Professor\n","\n","School of Earth and Sustainability\n","\n","School of Informatics, Computing and Cyber Systems\n","\n","\n","Northern Arizona University, Flagstaff, AZ\n","\n","[Email](mailto:daniel.buscombe@nau.edu)\n","[Web](www.danielbuscombe.com)\n","[Google Scholar](https://scholar.google.com/citations?user=bwVl0NwAAAAJ&hl=en)"]},{"cell_type":"code","metadata":{"id":"CUqdW0w4VdAJ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}